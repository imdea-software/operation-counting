\section{Introduction}

\paragraph{Motivation}

Efficient implementations of (shared-memory) concurrent
objects such as semaphores, locks, atomic registers, and data structures (like
sets, stacks, and queues) are essential to modern computing. Several libraries
implementing operations for the manipulation of such objects are available
(e.g., \cite{}). Clients (or users) of these libraries assume that the latter
are \emph{conform} to \emph{reference implementations} where, typically
operations (or methods) are \emph{atomic}, as it helps apprehending the library
behaviors. However, in order to minimize synchronization overhead between
concurrent object invocations, implementors of concurrent objects need to relax
atomicity, allowing operations to be concurrently intertwined. Still,
implementors must ensure that this relaxation is fully transparent to the
client, that is, the interactions of the library with the client should indeed
be conform to his expectations (corresponding to the reference implementation).
This is a notoriously hard and error prune task. The necessary intricacy of
these implementations is indeed a breeding ground for insidious and
difficult-to-diagnose bugs. Accordingly, algorithmic methods for detecting
conformance violation between implementations of concurrent objets is in high
demand.

Conformance between libraries is formally captured by the concept of
\emph{observational refinement}: Given two libraries $L_1$ and $L_2$, each of
them implementing the operations of some concurrent object, $L_1$
\emph{refines} $L_2$ if and only if for every (library-client) program $P$,
every execution of $P$ invoking $L_1$ is also possible when $P$ invokes $L_2$
instead.

The challenge we address in this paper is to provide an efficient algorithmic
approach for automatic detection of refinement violations.

\paragraph{Observational refinement vs. history inclusion}

Naturally, automating the verification of observational refinement is quite
challenging. The most immediate obstacle arises from the quantification over
the infinitely many possible library-client programs. The first contribution of
this paper is to provide a \emph{precise} characterization of the observational
refinement problem between two libraries $L_1$ and $L_2$ as an inclusion
problem between two sets of (happen-before) partial orders on their operations,
defined independently from their execution contexts.

More precisely, we associate to each execution a partial order on its
operations, called \emph{history}, where an operation $o_1$ is considered to
happen before an operation $o_2$ if $o_1$ returns (terminates) before $o_2$ is
called (starts). We prove that a library $L_1$ refines another one $L_2$ if and
only if the set $H(L_1)$ of histories (associated with the executions) of $L_1$
is included in the set $H(L_2)$ of histories of $L_2$.

\paragraph{History inclusion vs. linearizability}

The characterization of observational refinement as a history inclusion problem
is a fundamental result that offers a fresh view for reasoning about this
problem. Indeed, the principal approach for tackling the problem of checking
observation refinement in the literature is based on checking
\emph{linearizability}~\cite{}, i.e., that every execution in $L_1$ can be
reordered into an execution of $L_2$ while preserving the order between return
and call actions. Actually, while linearizability implies observational
refinement \cite{}, we show that the converse does not hold in general.

In order to shed light on the subtle relation between these concepts, we
investigate the links between history inclusion and linearizability. We prove
that, interestingly, when $L_2$ is atomic (which is typically the case for
reference implementations as mentioned above), history inclusion (and therefore
observational refinement) between $L_1$ and $L_2$ is equivalent to
linearizability.

As we will see in the sequel, besides being a useful semantical means for
reasoning about observational refinement, our characterization in terms of
history inclusion leads to an efficient and powerful approach for detecting
refinement violations.

\paragraph{Complexity obstacles}

In fact, the problem of checking observational refinement is intrinsically
hard. Even for a single execution (of some library $L_1$), checking that its
history belongs to the set $H(L_2)$, for some fixed library $L_2$, is an
NP-hard problem \cite{}. As mentioned earlier, existing approaches for finding
refinement violations are based on finding linearizability bugs, and they do
that basically by enumerating all (exponentially many) possible linearizations
of executions, which can only be applied for a limited number of operation
invocations. Moreover, from our result in \cite{}, observational refinement is
in general undecidable. (The proof there concerns linearizability, but uses a
specification that can be seen as an atomic reference implementation.) To
overcome these decidability and complexity obstacles, we adopt an approach
based on parameterized under-approximations.

\paragraph{Parameterized approximation schema}

The approach we introduce in this paper is based on fundamental properties of
library executions and their histories. The basic idea is to consider a notion
of \emph{ weakening pre-order} $\preceq$ on histories as a means for
approximation. Roughly, a history $h_1$ is weaker than another history $h_2$,
written $h_1 \preceq h_2$, if $h_1$ is obtained from $h_2$ by relaxing some of
its order constraints. An important fact that makes this idea exploitable is
that, for every library $L$ (in some formally well defined sense), the set of
its histories $H(L)$ is \emph{downward closed} under $\preceq$, that is, if $h
\in H(L)$, then the same holds for every $h' \preceq h$. Indeed, this fact
leads to the principle of considering for histories $h \in H(L_1)$
approximations $h' \preceq h$, such that checking $h' \in H(L_2)$ is tractable.
If $h' \not\in H(L_2)$, we know that we also have $h \not\in H(L_2)$, since
$H(L_2)$ is $\preceq$-downward closed, and therefore a refinement violation is
found.

Then, the challenge we undertake is to define an approximation schema based on
defining a series of functions $A_k$, parameterized with $k \in \mathbb{N}$,
such that for every $h$, we have (1) $A_k (h) \preceq h$, and (2) the test
$A_k(h) \in H(L_2)$ is decidable in \emph{polynomial time} (w.r.t. size of
$h$). Moreover, the schema should be \emph{complete} in the sense that there
must be a $k$ such that $h \preceq A_k(h)$, which means that if a violation
exists, it will be captured for a large enough parameter. Finally, and quite
importantly, we seek for an approximation schema that is natural and easy to
implement, and which is able to catch refinement violations with small
parameter values.


 
\paragraph{Bounded interval-length histories}

In the paper, we provide such an approximation schema, exploiting a fundamental
property of the executions of shared-memory libraries. In fact, we show that
histories of such executions are not arbitrary orders, but particular orders
called \emph{interval orders} \cite{}. The main property of these orders is
that they admit a \emph{canonical representation} where each element $o$ is
mapped to an interger-bounded interval $I(o)$ such that for every two elements
$o_1$ and $o_2$, $o_1$ is before $o_2$ if and only if $I(o_1)$ ends before
$I(o_2)$ starts \cite{}.

Also, based on this, interval orders have a (known) notion of \emph{length}
which corresponds to the maximal upper-bound of an interval in their canonical
representation \cite{}. This leads us to a natural approach for defining a
weakening-based approximation schema based on functions $A_k$ where the bound
$k$ corresponds to the notion of length mentioned above; for each $k \in
\mathbb{N}$, the function $A_k$ maps each history $h$ to a ($\preceq$-)weaker
history $h'$ of interval-length $k$. In this paper, we consider approximation
functions that keep precise the last $k$ interval bounds, and abstract all the
previous ones with equality.

\paragraph{Reduction to reachability using counting representations}

We show in the paper that interval-length bounding is a tractable approach for
refinement checking, that can be implemented efficiently. A key idea for that
is to use \emph{counting representations} for bounded interval-length
histories: each interval is represented by a counter corresponding to the
number of elements mapped to this interval in the canonical representation.
(Notice that there might be an unbounded number of elements mapped to a same
interval.) Indeed, representing histories as vectors of integers opens the door
to symbolic manipulation of sets of histories using arithmetical constraints.
In fact, we introduce for that a simple logic, called OCL (for Operation
Counting Logic), that is suitable for reasoning about libraries implementing
common concurrent objects, and for which checking if a given history satisfies
a formula can be done in \emph{polynomial time}. Moreover, using counting
representations provide a simple way for implementing a monitor $P_k$ for
executions having $k$-bounded interval-length histories. In fact, we define a
\emph{polynomial time} algorithm that, given an execution, builds a
($k$-bounded interval-length) approximation history of it. Therefore, bounded
interval-length refinement checking between $L_1$ and $L_2$ can be reduced to a
\emph{reachability (or invariant) checking} problem in $P_k$ composed with
$L_1$, provided that the set of histories $A_k(H(L_2))$ is effectively defined
in OCL. We show that this is the case for reference implementations of the
usual concurrent objects such as collection objects (like stacks or queues),
semaphores, locks, etc.

\paragraph{Demonstrating feasability}

The reduction of interval-bounded refinement checking to reachability checking
can be exploited in several ways. We demonstrate in the paper that our approach
is feasible both with a dynamic and a static state space exploration strategy.

In the dynamic case, we show experimentally two remarkable facts: (1) our
approach is scalable: its overhead is low and does not increase with the length
of computations, whereas the overhead of the standard approach (used in
existing tools such as Linup \cite{}) that is based on enumerating
linearizations of executions explodes exponentially. (2) our approach is
efficient and well suited for catching refinement violations: most of these
violations in practice are detected with small bounds, i.e., for $k$ ranging
from 0 to $2$, and only few (marginal) cases need greater bounds such as 3 or
4. On this issue, we were actually able to prove that for the common data
structures of sets, stacks, and queues, refinement checking for order
constraints can actually be reduced (without loss of completeness) to bounded
refinement checking with small cut-off bounds, namely, 2, 3, and 2 respectively.

In the static case, using the fact that sets of histories can be represented
using arithmetical constraints, we can, for the first time check the existence
of refinement violations using existing tools for reachability analysis of
concurrent programs such as CSeq \cite{} (based on CBMC \cite{}) that are based
on efficient symbolic encodings of sets of computations and the use of SMT
solvers.

\paragraph{Summary}

To summarize, the papers presents the following results and contributions,
leading to the definition of a simple and scalable algorithm for detecting
refinement violations.

*** List of contributions ***
